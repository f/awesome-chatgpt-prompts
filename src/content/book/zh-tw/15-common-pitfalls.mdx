即使是經驗豐富的提示詞工程師也會陷入一些可預見的陷阱。好訊息是：一旦你認識到這些模式，就很容易避免它們。本章將詳細介紹最常見的陷阱，解釋它們發生的原因，並為你提供具體的避免策略。

<Callout type="warning" title="為什麼瞭解陷阱很重要">
一個陷阱就可能把強大的 AI 變成令人沮喪的工具。理解這些模式往往是"AI 對我沒用"和"AI 改變了我的工作流程"之間的關鍵區別。
</Callout>

## 模糊陷阱

**模式**：你知道自己想要什麼，所以假設 AI 也能理解。但模糊的提示詞會產生模糊的結果。

<Compare
  before={{ label: "模糊的提示詞", content: "寫一些關於行銷的內容。" }}
  after={{ label: "具體的提示詞", content: "寫一篇300字的 LinkedIn 帖子，關於品牌一致性對 B2B SaaS 公司的重要性，目標受眾是行銷經理。使用專業但平易近人的語氣。包含一個具體的例子。" }}
/>

**為什麼會發生**：當我們認為某些細節是"顯而易見"的時候，我們自然會跳過它們。但對你來說顯而易見的事情，對於一個不瞭解你的情況、受眾或目標的模型來說並不明顯。

<TryIt
  title="具體性改進器"
  description="將一個模糊的提示詞變得具體。注意新增細節如何改變結果的品質。"
  prompt={`我有一個需要改進的模糊提示詞。

原始模糊提示詞："\${vaguePrompt}"

通過新增以下內容使這個提示詞變得具體：
1. **受眾**：誰會閱讀/使用這個？
2. **格式**：應該有什麼結構？
3. **長度**：應該多長？
4. **語氣**：什麼樣的聲音或風格？
5. **背景**：情況或目的是什麼？
6. **約束**：有什麼必須包含或必須避免的？

重寫提示詞，包含所有這些細節。`}
/>

## 過載陷阱

**模式**：你試圖在一個提示詞中獲得所有內容——全面、有趣、專業、適合初學者、進階、SEO 優化，而且還要簡短。結果呢？AI 遺漏了一半的要求，或者產生了混亂的內容。

<Compare
  before={{ label: "過載的提示詞", content: "寫一篇關於 AI 的部落格文章，要 SEO 優化，包含程式碼範例，要有趣但專業，面向初學者但也有進階技巧，應該是500字但要全面，提到我們的產品，還要有呼籲行動..." }}
  after={{ label: "專注的提示詞", content: "寫一篇500字的部落格文章，向初學者介紹 AI。\n\n要求：\n1. 清楚地解釋一個核心概念\n2. 包含一個簡單的程式碼範例\n3. 以呼籲行動結尾\n\n語氣：專業但平易近人" }}
/>

**為什麼會發生**：害怕多次互動，或者想一次性"把所有東西都說出來"。但認知過載對 AI 的影響就像對人類一樣——太多相互競爭的要求會導致遺漏。

<InfoGrid items={[
  { label: "限制要求數量", description: "每個提示詞堅持3-5個關鍵要求", example: "專注於：受眾、格式、長度、一個關鍵約束", exampleType: "text", color: "green" },
  { label: "使用編號列表", description: "結構使優先級更清晰", example: "1. 必須有 X，2. 應該有 Y，3. 最好有 Z", exampleType: "text", color: "green" },
  { label: "鏈式提示詞", description: "將複雜任務分解為多個步驟", example: "首先：大綱。然後：起草第1節。然後：起草第2節。", exampleType: "text", color: "green" },
  { label: "無情地優先排序", description: "什麼是必要的 vs. 錦上添花的？", example: "如果我只能做好一件事，那會是什麼？", color: "green" }
]} />

<Callout type="tip" title="學習提示詞鏈">
當單個提示詞變得過載時，[提示詞鏈](/book/11-prompt-chaining)通常是解決方案。將複雜任務分解為一系列專注的提示詞，每個步驟都建立在前一個步驟的基礎上。
</Callout>

## 假設陷阱

**模式**：你引用"之前"的內容，或假設 AI 知道你的專案、公司或之前的對話。它並不知道。

<Compare
  before={{ label: "假設有上下文", content: "更新我之前給你看的函式，新增錯誤處理。" }}
  after={{ label: "提供上下文", content: "更新這個函式以新增錯誤處理：\n\n```python\ndef calculate_total(items):\n    return sum(item.price for item in items)\n```\n\n為空列表和無效專案新增 try/except。" }}
/>

**為什麼會發生**：與 AI 對話感覺像是在和同事交談。但與同事不同，大多數 AI 模型在會話之間沒有持久記憶——每次對話都是從頭開始。

<TryIt
  title="上下文完整性檢查"
  description="使用這個來驗證你的提示詞在發送前包含所有必要的上下文。"
  prompt={`檢查這個提示詞是否缺少上下文：

"\${promptToCheck}"

檢查以下內容：
1. **引用但未包含**：是否提到"程式碼"、"文件"、"之前"或"上面"但沒有包含實際內容？

2. **假設的知識**：是否假設了對特定專案、公司或情況的瞭解？

3. **隱含要求**：是否有對格式、長度或風格的未說明期望？

4. **缺失背景**：一個聰明的陌生人能理解所問的問題嗎？

列出缺失的內容並建議如何新增。`}
/>

## 引導性問題陷阱

**模式**：你以一種嵌入假設的方式提出問題，得到的是確認而不是洞察。

<Compare
  before={{ label: "引導性問題", content: "為什麼 Python 是資料科學最好的程式語言？" }}
  after={{ label: "中立問題", content: "比較 Python、R 和 Julia 在資料科學工作中的應用。每種語言的優缺點是什麼？什麼時候你會選擇一種而不是其他的？" }}
/>

**為什麼會發生**：我們往往尋求確認，而不是資訊。我們的措辭會不自覺地推向我們期望或想要的答案。

<TryIt
  title="偏見檢測器"
  description="檢查你的提示詞是否有隱藏的偏見和引導性語言。"
  prompt={`分析這個提示詞中的偏見和引導性語言：

"\${promptToAnalyze}"

檢查以下內容：
1. **嵌入的假設**：問題是否假設某事是真的？
2. **引導性措辭**："為什麼 X 好？"是否假設 X 是好的？
3. **缺少替代方案**：是否忽略了其他可能性？
4. **尋求確認**：是在尋求驗證而不是分析嗎？

重寫提示詞使其中立和開放。`}
/>

## 完全信任陷阱

**模式**：AI 的回覆聽起來自信且權威，所以你不加驗證就接受了。但自信並不等於準確。

<InfoGrid items={[
  { label: "未審核的內容", description: "發佈 AI 生成的文本而不進行事實核查", example: "帶有虛構統計資料或假引用的部落格文章", exampleType: "text", color: "red" },
  { label: "未測試的程式碼", description: "在生產環境中使用未經測試的 AI 程式碼", example: "安全漏洞、邊緣情況失敗、微妙的 bug", exampleType: "text", color: "red" },
  { label: "盲目決策", description: "僅基於 AI 分析做出重要決定", example: "基於幻覺市場資料的商業策略", exampleType: "text", color: "red" }
]} />

**為什麼會發生**：AI 即使完全錯誤也聽起來很自信。我們也容易產生"自動化偏誤"——過度信任電腦輸出的傾向。

<TryIt
  title="驗證提示詞"
  description="使用這個讓 AI 標記自己的不確定性和潛在錯誤。"
  prompt={`我需要關於以下主題的資訊：\${topic}

重要提示：在你的回覆之後，新增一個名為"驗證說明"的部分，包括：

1. **信心度**：你對這些資訊有多確定？（高/中/低）

2. **潛在錯誤**：這個回覆中哪些部分最可能是錯誤的或過時的？

3. **需要驗證的內容**：使用者應該獨立核實哪些具體宣告？

4. **可查閱的來源**：使用者可以在哪裡驗證這些資訊？

對侷限性要誠實。標記不確定性比對錯誤的事情表現出自信要好。`}
/>

## 一次性陷阱

**模式**：你發送一個提示詞，得到一個平庸的結果，然後得出結論說 AI "不適合"你的使用案例。但優秀的結果幾乎總是需要迭代。

<Compare
  before={{ label: "一次性思維", content: "平庸的輸出 → \"AI 做不了這個\" → 放棄" }}
  after={{ label: "迭代思維", content: "平庸的輸出 → 分析問題所在 → 改進提示詞 → 更好的輸出 → 再次改進 → 優秀的輸出" }}
/>

**為什麼會發生**：我們期望 AI 第一次就能讀懂我們的想法。我們不期望 Google 搜尋需要迭代，但卻期望 AI 完美無缺。

<TryIt
  title="迭代助手"
  description="當你的第一個結果不對時，使用這個來系統地改進它。"
  prompt={`我原來的提示詞是：
"\${originalPrompt}"

我得到的輸出是：
"\${outputReceived}"

問題在於：
"\${whatIsWrong}"

幫我迭代：

1. **診斷**：為什麼原來的提示詞產生了這個結果？

2. **缺失元素**：我應該明確說明但沒有說明的是什麼？

3. **修改後的提示詞**：重寫我的提示詞來解決這些問題。

4. **需要注意的事項**：我應該在新輸出中檢查什麼？`}
/>

## 格式忽視陷阱

**模式**：你專注於讓 AI 說什麼，但忘記指定它應該如何格式化。然後當你需要 JSON 時得到了敘述文字，或者當你需要要點列表時得到了一堵文字牆。

<Compare
  before={{ label: "未指定格式", content: "從這段文本中提取關鍵資料。" }}
  after={{ label: "指定格式", content: "從這段文本中提取關鍵資料，以 JSON 格式輸出：\n\n{\n  \"name\": string,\n  \"date\": \"YYYY-MM-DD\",\n  \"amount\": number,\n  \"category\": string\n}\n\n只返回 JSON，不要解釋。" }}
/>

**為什麼會發生**：我們專注於內容而不是結構。但如果你需要程式化地解析輸出，或將其貼上到特定位置，格式和內容一樣重要。

<TryIt
  title="格式規範生成器"
  description="為你需要的任何輸出類型生成清晰的格式規範。"
  prompt={`我需要特定格式的 AI 輸出。

**我要求的內容**：\${taskDescription}
**我將如何使用輸出**：\${intendedUse}
**首選格式**：\${formatType}（JSON、Markdown、CSV、要點列表等）

生成一個我可以新增到提示詞中的格式規範，包括：

1. **精確結構**：包含欄位名稱和類型
2. **範例輸出**：展示格式
3. **約束條件**（例如，"只返回 JSON，不要解釋"）
4. **邊緣情況**（如果資料缺失應該輸出什麼）`}
/>

## 上下文窗口陷阱

**模式**：你貼上一個巨大的文件並期望得到全面的分析。但模型有其限制——它們可能會截斷、失去焦點，或在長輸入中遺漏重要細節。

<InfoGrid items={[
  { label: "瞭解你的限制", description: "不同的模型有不同的上下文窗口", example: "GPT-4: 128K tokens, Claude: 200K tokens, Gemini: 1M tokens", exampleType: "text", color: "blue" },
  { label: "分塊處理大輸入", description: "將文件分成可管理的部分", example: "分別分析各章節，然後綜合", exampleType: "text", color: "blue" },
  { label: "前置重要資訊", description: "將關鍵上下文放在提示詞的開頭", example: "關鍵要求在前，背景細節在後", exampleType: "text", color: "blue" },
  { label: "去除冗餘", description: "刪除不必要的上下文", example: "你真的需要整個文件，還是隻需要相關部分？", exampleType: "text", color: "blue" }
]} />

<TryIt
  title="文件分塊策略"
  description="獲取處理超出上下文限制的文件的策略。"
  prompt={`我有一個需要分析的大文件：

**文件類型**：\${documentType}
**大約長度**：\${documentLength}
**我需要提取/分析的內容**：\${analysisGoal}
**我使用的模型**：\${modelName}

建立一個分塊策略：

1. **如何劃分**：此類文件的邏輯中斷點
2. **每個塊中包含什麼**：獨立分析所需的上下文
3. **如何綜合**：組合多個塊的結果
4. **需要注意的事項**：可能跨塊的資訊`}
/>

## 擬人化陷阱

**模式**：你把 AI 當作人類同事對待——期望它"喜歡"任務、記住你或關心結果。它並不會。

<Compare
  before={{ label: "擬人化", content: "我相信你會喜歡這個創意專案！我知道你喜歡幫助別人，這對我個人來說真的很重要。" }}
  after={{ label: "清晰直接", content: "根據以下規格寫一個創意短篇故事：\n- 類型：科幻\n- 長度：500字\n- 語氣：充滿希望\n- 必須包含：一個反轉結局" }}
/>

**為什麼會發生**：AI 的回覆如此像人類，以至於我們自然會陷入社交模式。但情感訴求不會讓 AI 更努力——清晰的指令才會。

<Callout type="info" title="什麼才真正有幫助">
與其進行情感訴求，不如專注於：清晰的要求、好的範例、具體的約束和明確的成功標準。這些能改善輸出。"請真的努力嘗試"則不能。
</Callout>

## 安全忽視陷阱

**模式**：在急於讓事情運轉起來的過程中，你在提示詞中包含了敏感資訊——API 金鑰、密碼、個人資料或專有資訊。

<InfoGrid items={[
  { label: "提示詞中的金鑰", description: "API 金鑰、密碼、token 貼上到提示詞中", example: "\"使用這個 API 金鑰：sk-abc123...\"", color: "red" },
  { label: "個人資料", description: "包含發送到第三方伺服器的個人身份資訊", example: "提示詞中的客戶姓名、電子郵件、地址", exampleType: "text", color: "red" },
  { label: "未清理的使用者輸入", description: "將使用者輸入直接傳遞到提示詞中", example: "提示詞注入漏洞", exampleType: "text", color: "red" },
  { label: "專有資訊", description: "商業機密或機密資料", example: "內部策略、未發佈的產品細節", exampleType: "text", color: "red" }
]} />

**為什麼會發生**：專注於功能而忽視安全。但請記住：提示詞通常發送到外部伺服器，可能會被記錄，並可能用於訓練。

<TryIt
  title="安全審查"
  description="在發送前檢查你的提示詞是否存在安全問題。"
  prompt={`審查此提示詞的安全問題：

"\${promptToReview}"

檢查以下內容：

1. **暴露的金鑰**：API 金鑰、密碼、token、憑證
2. **個人資料**：姓名、電子郵件、地址、電話號碼、身分證字號
3. **專有資訊**：商業機密、內部策略、機密資料
4. **注入風險**：可能操縱提示詞的使用者輸入

對於發現的每個問題：
- 解釋風險
- 建議如何編輯或保護資訊
- 推薦更安全的替代方案`}
/>

## 幻覺忽視陷阱

**模式**：你要求引用、統計資料或具體事實，並假設它們是真實的，因為 AI 自信地陳述了它們。但 AI 經常編造聽起來可信的資訊。

<Compare
  before={{ label: "盲目信任", content: "給我5個關於遠端工作生產力的統計資料和來源。" }}
  after={{ label: "承認侷限性", content: "關於遠端工作生產力，我們知道些什麼？對於你提到的任何統計資料，請說明它們是已證實的發現還是更不確定的。我會獨立驗證任何具體數字。" }}
/>

**為什麼會發生**：AI 生成的文本聽起來很權威。它不"知道"自己什麼時候在編造——它是在預測可能的文本，而不是檢索經過驗證的事實。

<TryIt
  title="抗幻覺查詢"
  description="建構你的提示詞以最小化幻覺風險並標記不確定性。"
  prompt={`我需要關於以下主題的資訊：\${topic}

請遵循這些指南以最小化錯誤：

1. **堅持使用公認的事實**。避免難以驗證的晦澀說法。

2. **標記不確定性**。如果你對某事不確定，請說"我認為..."或"這可能需要驗證..."

3. **不要編造來源**。除非你確定它們存在，否則不要引用具體的論文、書籍或 URL。相反，描述在哪裡可以找到這類資訊。

4. **承認知識限制**。如果我的問題涉及你訓練資料之後的事件，請說明。

5. **區分事實和推斷**。清楚地區分"X 是真的"和"基於 Y，X 可能是真的"。

現在，請記住這些指南：\${actualQuestion}`}
/>

## 發送前檢查清單

在發送任何重要的提示詞之前，快速瀏覽這個檢查清單：

<Checklist
  title="提示詞品質檢查"
  items={[
    { text: "是否足夠具體？（不模糊）" },
    { text: "是否專注？（沒有過載要求）" },
    { text: "是否包含所有必要的上下文？" },
    { text: "問題是否中立？（不具引導性）" },
    { text: "是否指定了輸出格式？" },
    { text: "輸入是否在上下文限制內？" },
    { text: "是否有安全顧慮？" },
    { text: "我是否準備好驗證輸出？" },
    { text: "如果需要，我是否準備好迭代？" }
  ]}
/>

<Quiz
  question="在使用 AI 做重要決策時，最危險的陷阱是什麼？"
  options={[
    "使用模糊的提示詞",
    "不加驗證地信任 AI 輸出",
    "不指定輸出格式",
    "在提示詞中過載要求"
  ]}
  correctIndex={1}
  explanation="雖然所有陷阱都會造成問題，但不加驗證地信任 AI 輸出是最危險的，因為它可能導致發佈虛假資訊、部署有漏洞的程式碼，或基於幻覺資料做出決策。AI 即使完全錯誤也聽起來很自信，這使得驗證對任何重要使用案例都至關重要。"
/>

## 分析你的提示詞

使用 AI 獲取關於提示詞品質的即時反饋。貼上任何提示詞並獲得詳細分析：

<PromptAnalyzer
  title="提示詞品質分析器"
  description="獲取 AI 驅動的關於清晰度、具體性的反饋，以及改進建議"
  defaultPrompt="幫我處理我的程式碼"
/>

## 偵錯這個提示詞

你能發現這個提示詞有什麼問題嗎？

<PromptDebugger
  title="找出陷阱"
  badPrompt="寫一篇關於科技的部落格文章，要 SEO 優化帶關鍵詞，還要有趣但專業，包含程式碼範例，面向初學者但有進階技巧，提到我們的產品 TechCo，有社會認同和呼籲行動，500字但要全面。"
  badOutput="這是一篇關於科技的部落格文章草稿...

[通用的、不聚焦的內容，試圖做所有事情但什麼都做不好。語氣在隨意和技術之間尷尬地轉換。遺漏了一半的要求。]"
  options={[
    { id: "vague", label: "提示詞太模糊", isCorrect: false, explanation: "實際上，這個提示詞有很多具體的要求。問題恰恰相反——要求太多，而不是太少。" },
    { id: "overload", label: "提示詞過載了太多相互競爭的要求", isCorrect: true, explanation: "正確！這個提示詞要求 SEO + 有趣 + 專業 + 程式碼 + 初學者 + 進階 + 產品提及 + 社會認同 + 呼籲行動 + 長度限制。這是10多個相互競爭的要求！AI 無法滿足所有要求，所以每件事都做得很一般。解決方案：將其分解為多個專注的提示詞。" },
    { id: "format", label: "沒有指定輸出格式", isCorrect: false, explanation: "雖然更具體的格式會有幫助，但主要問題是要求過載。你無法通過格式化來解決要求太多的問題。" }, 
    { id: "context", label: "上下文不夠", isCorrect: false, explanation: "這個提示詞實際上有很多上下文——也許太多了！問題在於它試圖同時滿足太多目標。" }
  ]}
  hint="數一數這個單一提示詞中塞進了多少不同的要求。"
/>
