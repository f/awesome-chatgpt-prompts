Die meiste Zeit der Geschichte arbeiteten Computer mit jeweils einem Datentyp: Text in einem Programm, Bilder in einem anderen, Audio woanders. Aber Menschen erleben die Welt nicht so. Wir sehen, hören, lesen und sprechen gleichzeitig und kombinieren all diese Eingaben, um unsere Umgebung zu verstehen.

**Multimodale KI** ändert alles. Diese Modelle können mehrere Informationstypen zusammen verarbeiten—ein Bild analysieren, während sie deine Frage dazu lesen, oder Bilder aus deinen Textbeschreibungen generieren. Dieses Kapitel lehrt dich, wie du effektiv mit diesen leistungsstarken Systemen kommunizierst.

<Callout type="info" title="Was bedeutet Multimodal?">
"Multi" bedeutet viele, und "modal" bezieht sich auf Modi oder Datentypen. Ein multimodales Modell kann mit mehreren Modalitäten arbeiten: Text, Bilder, Audio, Video oder sogar Code. Statt separater Tools für jeden Typ versteht ein Modell alle zusammen.
</Callout>

## Warum Multimodal wichtig ist

Traditionelle KI erforderte, dass du alles in Worten beschreibst. Willst du etwas über ein Bild fragen? Du müsstest es erst beschreiben. Ein Dokument analysieren? Du müsstest es manuell transkribieren. Multimodale Modelle beseitigen diese Barrieren.

<InfoGrid items={[
  { label: "Sehen und Verstehen", description: "Lade ein Bild hoch und stelle Fragen direkt—keine Beschreibung nötig", example: "\"Was stimmt nicht mit diesem Schaltplan?\"", color: "blue" },
  { label: "Aus Worten erschaffen", description: "Beschreibe was du willst und generiere Bilder, Audio oder Video", example: "\"Ein Sonnenuntergang über Bergen im Aquarellstil\"", color: "purple" },
  { label: "Alles kombinieren", description: "Mische Text, Bilder und andere Medien in einem Gespräch", example: "\"Vergleiche diese beiden Designs und sag mir, welches besser für Mobile ist\"", color: "green" },
  { label: "Dokumente analysieren", description: "Informationen aus Fotos von Dokumenten, Quittungen oder Screenshots extrahieren", example: "\"Extrahiere alle Positionen aus diesem Rechnungsfoto\"", color: "amber" }
]} />

## Warum Prompting bei Multimodal noch wichtiger ist

Bei Nur-Text-Modellen erhält die KI genau das, was du tippst. Aber bei multimodalen Modellen muss die KI visuelle oder Audioinformationen interpretieren—und Interpretation erfordert Anleitung.

<Compare 
  before={{ label: "Vager multimodaler Prompt", content: "Was siehst du in diesem Bild?\n\n[Bild eines komplexen Dashboards]" }}
  after={{ label: "Angeleiteter multimodaler Prompt", content: "Dies ist ein Screenshot unseres Analytics-Dashboards. Konzentriere dich auf:\n1. Das Conversion-Rate-Diagramm oben rechts\n2. Alle Fehlerindikatoren oder Warnungen\n3. Ob die Daten normal oder anomal aussehen\n\n[Bild eines komplexen Dashboards]" }}
/>

**Ohne Anleitung** beschreibt das Modell vielleicht Farben, Layout oder irrelevante Details. **Mit Anleitung** fokussiert es sich auf das, was dir wirklich wichtig ist.

<Callout type="warning" title="Die Interpretationslücke">
Wenn du ein Bild betrachtest, weißt du sofort basierend auf deinem Kontext und deinen Zielen, was wichtig ist. Die KI hat diesen Kontext nicht, es sei denn, du lieferst ihn. Ein Foto von einem Riss in der Wand könnte sein: ein baustatisches Problem, eine künstlerische Textur oder irrelevanter Hintergrund. Dein Prompt bestimmt, wie die KI es interpretiert.
</Callout>

## Die Multimodale Landschaft

Verschiedene Modelle haben verschiedene Fähigkeiten. Hier ist, was 2025 verfügbar ist:

### Verstehende Modelle (Eingabe → Analyse)

Diese Modelle akzeptieren verschiedene Medientypen und produzieren Textanalysen oder Antworten.

<InfoGrid items={[
  { label: "GPT-4o / GPT-5", description: "Text + Bilder + Audio → Text. OpenAIs Flaggschiff mit 128K Kontext, starken kreativen und Reasoning-Fähigkeiten, reduzierte Halluzinationsraten.", color: "green" },
  { label: "Claude 4 Sonnet/Opus", description: "Text + Bilder → Text. Anthropics sicherheitsfokussiertes Modell mit fortgeschrittenem Reasoning, ausgezeichnet für Coding und komplexe mehrstufige Aufgaben.", color: "purple" },
  { label: "Gemini 2.5", description: "Text + Bilder + Audio + Video → Text. Googles Modell mit 1M Token Kontext, Selbst-Fakten-Check, schnelle Verarbeitung für Coding und Recherche.", color: "blue" },
  { label: "LLaMA 4 Scout", description: "Text + Bilder + Video → Text. Metas Open-Source-Modell mit massivem 10M Token Kontext für lange Dokumente und Codebases.", color: "cyan" },
  { label: "Grok 4", description: "Text + Bilder → Text. xAIs Modell mit Echtzeit-Datenzugang und Social-Media-Integration für aktuelle Antworten.", color: "red" }
]} />

### Generierungsmodelle (Text → Medien)

Diese Modelle erstellen Bilder, Audio oder Video aus Textbeschreibungen.

<InfoGrid items={[
  { label: "DALL-E 3", description: "Text → Bilder. OpenAIs Bildgenerator mit hoher Genauigkeit zu Prompt-Beschreibungen.", color: "amber" },
  { label: "Midjourney", description: "Text + Bilder → Bilder. Bekannt für künstlerische Qualität, Stilkontrolle und ästhetische Ausgaben.", color: "pink" },
  { label: "Sora", description: "Text → Video. OpenAIs Videogenerierungsmodell zum Erstellen von Clips aus Beschreibungen.", color: "red" },
  { label: "Whisper", description: "Audio → Text. OpenAIs Speech-to-Text mit hoher Genauigkeit über Sprachen hinweg.", color: "cyan" }
]} />

<Callout type="info" title="Schnelle Evolution">
Die multimodale Landschaft ändert sich schnell. Neue Modelle werden häufig veröffentlicht, und bestehende Modelle gewinnen durch Updates neue Fähigkeiten. Prüfe immer die neueste Dokumentation für aktuelle Features und Einschränkungen.
</Callout>

## Bildverständnis-Prompts

Der häufigste multimodale Anwendungsfall ist, KI zu bitten, Bilder zu analysieren. Der Schlüssel ist, Kontext darüber zu liefern, was du brauchst.

### Grundlegende Bildanalyse

Beginne mit einer klaren Anfragestruktur. Sage dem Modell, auf welche Aspekte es sich konzentrieren soll.

<TryIt 
  title="Strukturierte Bildanalyse"
  description="Dieser Prompt bietet ein klares Framework für Bildanalyse. Das Modell weiß genau, welche Informationen du brauchst."
  prompt={`Analysiere dieses Bild und beschreibe:

1. **Hauptmotiv**: Was ist der primäre Fokus dieses Bildes?
2. **Umgebung**: Wo scheint das zu sein? (innen/außen, Ortstyp)
3. **Stimmung**: Welchen emotionalen Ton oder Atmosphäre vermittelt es?
4. **Textinhalt**: Sichtbarer Text, Schilder oder Labels?
5. **Bemerkenswerte Details**: Was könnte jemand auf den ersten Blick übersehen?
6. **Technische Qualität**: Wie sind Beleuchtung, Fokus und Komposition?

[Füge das zu analysierende Bild ein oder beschreibe es]

Bildbeschreibung oder URL: \${bildBeschreibung}`}
/>

### Strukturierte Ausgabe für Bilder

Wenn du Bildanalyse programmatisch verarbeiten musst, fordere JSON-Ausgabe an.

<TryIt 
  title="JSON-Bildanalyse"
  description="Erhalte strukturierte Daten aus der Bildanalyse, die leicht zu parsen und in Anwendungen zu verwenden sind."
  prompt={`Analysiere dieses Bild und gib ein JSON-Objekt mit folgender Struktur zurück:

{
  "zusammenfassung": "Ein-Satz-Beschreibung",
  "objekte": ["Liste der sichtbaren Hauptobjekte"],
  "personen": {
    "anzahl": "Zahl oder 'keine'",
    "aktivitaeten": ["Was sie tun, falls welche"]
  },
  "erkannter_text": ["Im Bild sichtbarer Text"],
  "farben": {
    "dominant": ["Top 3 Farben"],
    "stimmung": "Warm/Kühl/Neutral"
  },
  "umgebung": {
    "typ": "innen/außen/unbekannt",
    "beschreibung": "Spezifischere Ortsbeschreibung"
  },
  "technisch": {
    "qualitaet": "hoch/mittel/niedrig",
    "beleuchtung": "Beschreibung der Beleuchtung",
    "komposition": "Beschreibung von Rahmung/Komposition"
  },
  "konfidenz": "hoch/mittel/niedrig"
}

Zu analysierendes Bild: \${bildBeschreibung}`}
/>

### Vergleichende Analyse

Der Vergleich mehrerer Bilder erfordert klare Beschriftung und spezifische Vergleichskriterien.

<TryIt 
  title="Bildvergleich"
  description="Vergleiche zwei oder mehr Bilder mit spezifischen Kriterien, die für deine Entscheidung wichtig sind."
  prompt={`Vergleiche diese Bilder für \${zweck}:

**Bild A**: \${bildA}
**Bild B**: \${bildB}

Analysiere jedes Bild nach diesen Kriterien:
1. \${kriterium1} (Wichtigkeit: hoch)
2. \${kriterium2} (Wichtigkeit: mittel)  
3. \${kriterium3} (Wichtigkeit: niedrig)

Liefere:
- Seite-an-Seite-Vergleich für jedes Kriterium
- Stärken und Schwächen von jedem
- Klare Empfehlung mit Begründung
- Alle Bedenken oder Vorbehalte`}
/>

## Dokument- und Screenshot-Analyse

Eine der praktischsten Anwendungen multimodaler KI ist das Analysieren von Dokumenten, Screenshots und UI-Elementen. Das spart Stunden manueller Transkription und Überprüfung.

### Dokumentenextraktion

Gescannte Dokumente, Fotos von Quittungen und PDFs als Bilder können alle verarbeitet werden. Der Schlüssel ist, dem Modell zu sagen, um welchen Dokumenttyp es sich handelt und welche Informationen du brauchst.

<TryIt 
  title="Dokumenten-Daten-Extraktor"
  description="Extrahiere strukturierte Daten aus Fotos von Dokumenten, Quittungen, Rechnungen oder Formularen."
  prompt={`Dies ist ein Foto/Scan eines \${dokumentTyp}.

Extrahiere alle Informationen in strukturiertes JSON-Format:

{
  "dokument_typ": "erkannter Typ",
  "datum": "falls vorhanden",
  "schluessel_felder": {
    "feldname": "wert"
  },
  "positionen": [
    {"beschreibung": "", "betrag": ""}
  ],
  "summen": {
    "zwischensumme": "",
    "steuer": "",
    "gesamt": ""
  },
  "handschriftliche_notizen": ["handgeschriebener Text"],
  "unklare_bereiche": ["schwer lesbare Bereiche"],
  "konfidenz": "hoch/mittel/niedrig"
}

WICHTIG: Wenn Text unklar ist, notiere es in "unklare_bereiche" statt zu raten. Markiere Konfidenz als "niedrig" wenn signifikante Teile schwer lesbar waren.

Dokumentbeschreibung: \${dokumentBeschreibung}`}
/>

### Screenshot- und UI-Analyse

Screenshots sind Goldgruben für Debugging, UX-Review und Dokumentation. Leite die KI an, sich auf das Wichtige zu konzentrieren.

<TryIt 
  title="UI/UX Screenshot-Analyzer"
  description="Erhalte detaillierte Analyse von Screenshots für Debugging, UX-Review oder Dokumentation."
  prompt={`Dies ist ein Screenshot von \${anwendungsName}.

Analysiere diese Oberfläche:

**Identifikation**
- Welcher Bildschirm/Seite/Zustand ist das?
- Was versucht der Benutzer hier wahrscheinlich zu erreichen?

**UI-Elemente**
- Wichtige interaktive Elemente (Buttons, Formulare, Menüs)
- Aktueller Zustand (etwas ausgewählt, ausgefüllt oder erweitert?)
- Fehlermeldungen, Warnungen oder Benachrichtigungen?

**UX-Bewertung**
- Ist das Layout klar und intuitiv?
- Verwirrende Elemente oder unklare Labels?
- Barrierefreiheits-Bedenken (Kontrast, Textgröße, etc.)?

**Erkannte Probleme**
- Visuelle Bugs oder Fehlausrichtungen?
- Abgeschnittener Text oder Overflow-Probleme?
- Inkonsistentes Styling?

Screenshot-Beschreibung: \${screenshotBeschreibung}`}
/>

## Bildgenerierungs-Prompts

Bilder aus Textbeschreibungen zu generieren ist eine Kunstform. Je spezifischer und strukturierter dein Prompt, desto näher kommt das Ergebnis deiner Vision.

### Die Anatomie eines Bild-Prompts

Effektive Bildgenerierungs-Prompts haben mehrere Komponenten:

<InfoGrid items={[
  { label: "Motiv", description: "Was ist der Hauptfokus des Bildes?", example: "Ein Golden Retriever, der im Herbstlaub spielt", color: "blue" },
  { label: "Stil", description: "Welcher künstlerische Stil oder welches Medium?", example: "Aquarellmalerei, digitale Kunst, fotorealistisch", color: "purple" },
  { label: "Komposition", description: "Wie ist die Szene arrangiert?", example: "Nahaufnahme-Portrait, weite Landschaft, Vogelperspektive", color: "green" },
  { label: "Beleuchtung", description: "Was ist die Lichtquelle und -qualität?", example: "Weiches Morgenlicht, dramatische Schatten, Neon-Glühen", color: "amber" },
  { label: "Stimmung", description: "Welches Gefühl soll es hervorrufen?", example: "Friedlich, energetisch, mysteriös, nostalgisch", color: "pink" },
  { label: "Details", description: "Spezifische einzuschließende oder zu vermeidende Elemente", example: "Einschließen: Blumen. Vermeiden: Text, Wasserzeichen", color: "cyan" }
]} />

### Grundlegende Bildgenerierung

<TryIt 
  title="Strukturierter Bild-Prompt"
  description="Verwende diese Vorlage, um detaillierte, spezifische Bildgenerierungs-Prompts zu erstellen."
  prompt={`Erstelle ein Bild mit diesen Spezifikationen:

**Motiv**: \${motiv}

**Stil**: \${stil}
**Medium**: \${medium} (z.B. Ölgemälde, digitale Kunst, Fotografie)

**Komposition**:
- Rahmung: \${rahmung} (Nahaufnahme, Halbtotale, Weitwinkel)
- Perspektive: \${perspektive} (Augenhöhe, Froschperspektive, von oben)
- Fokus: \${fokusBereich}

**Beleuchtung**:
- Quelle: \${lichtquelle}
- Qualität: \${lichtQualitaet} (weich, hart, diffus)
- Tageszeit: \${tageszeit}

**Farbpalette**: \${farben}

**Stimmung/Atmosphäre**: \${stimmung}

**Muss enthalten**: \${einschlussElemente}
**Muss vermeiden**: \${vermeidungsElemente}

**Technisch**: \${seitenverhaeltnis} Seitenverhältnis, hohe Qualität`}
/>

## Audio-Prompting

Audioverarbeitung eröffnet Transkription, Analyse und Verständnis gesprochener Inhalte. Der Schlüssel ist, Kontext darüber zu liefern, was das Audio enthält.

### Erweiterte Transkription

Grundlegende Transkription ist nur der Anfang. Mit guten Prompts kannst du Sprecheridentifikation, Zeitstempel und domänenspezifische Genauigkeit erhalten.

<TryIt 
  title="Intelligente Transkription"
  description="Erhalte genaue Transkriptionen mit Sprecher-Labels, Zeitstempeln und Behandlung unklarer Abschnitte."
  prompt={`Transkribiere diese Audioaufnahme.

**Kontext**: \${aufnahmeTyp} (Meeting, Interview, Podcast, Vorlesung, etc.)
**Erwartete Sprecher**: \${sprecherAnzahl} (\${sprecherRollen})
**Domäne**: \${domaene} (zu erwartende Fachbegriffe: \${fachbegriffe})

**Ausgabeformat**:
[00:00] **Sprecher 1 (Name/Rolle)**: Transkribierter Text hier.
[00:15] **Sprecher 2 (Name/Rolle)**: Ihre Antwort hier.

**Anweisungen**:
- Zeitstempel bei natürlichen Pausen einfügen (alle 30-60 Sekunden oder bei Sprecherwechsel)
- Unklare Abschnitte als [unverständlich] oder [unklar: beste Vermutung?] markieren
- Nicht-Sprache-Geräusche in Klammern notieren: [Lachen], [Telefon klingelt], [lange Pause]
- Füllwörter nur beibehalten, wenn sie bedeutungsvoll sind (äh, ähm können entfernt werden)
- Aktionspunkte oder Entscheidungen mit → Symbol markieren

Audio-Beschreibung: \${audioBeschreibung}`}
/>

## Best Practices für Multimodale Prompts

Großartige Ergebnisse von multimodaler KI zu erhalten erfordert, sowohl ihre Fähigkeiten als auch ihre Einschränkungen zu verstehen.

### Was multimodale Prompts effektiv macht

<InfoGrid items={[
  { label: "Kontext liefern", description: "Sage dem Modell, was das Medium ist und warum du es analysierst", example: "\"Dies ist ein Produktfoto für unsere E-Commerce-Seite...\"", color: "green" },
  { label: "Spezifisch sein", description: "Frage nach bestimmten Elementen statt nach allgemeinen Eindrücken", example: "\"Konzentriere dich auf die Preistabelle oben rechts\"", color: "green" },
  { label: "Orte referenzieren", description: "Zeige auf bestimmte Bereiche mit räumlicher Sprache", example: "\"Im unteren linken Quadranten...\"", color: "green" },
  { label: "Ziel nennen", description: "Erkläre, wofür du die Analyse verwenden wirst", example: "\"Ich muss entscheiden, ob dieses Bild für unsere Mobile-App funktioniert\"", color: "green" }
]} />

### Häufige Fallstricke vermeiden

<InfoGrid items={[
  { label: "Perfektes Sehen annehmen", description: "Modelle können kleine Details übersehen, besonders bei niedrigauflösenden Bildern", example: "Frage nicht nach 8pt-Text in einem komprimierten Screenshot", color: "red" },
  { label: "Perfektes OCR erwarten", description: "Handschrift, ungewöhnliche Schriften und komplexe Layouts können Fehler verursachen", example: "Überprüfe extrahierten Text von Quittungen und Formularen", color: "red" },
  { label: "Content-Richtlinien ignorieren", description: "Modelle haben Einschränkungen bei bestimmten Inhaltstypen", example: "Identifizieren keine spezifischen Personen oder analysieren unangemessene Inhalte", color: "red" },
  { label: "Verifikation überspringen", description: "Überprüfe immer kritische aus Medien extrahierte Informationen", example: "Überprüfe Zahlen, Daten und Namen aus Dokumentenextraktion doppelt", color: "red" }
]} />

<Quiz 
  question="Warum ist Prompting bei multimodalen Modellen WICHTIGER als bei Nur-Text-Modellen?"
  options={[
    "Multimodale Modelle sind weniger intelligent und brauchen mehr Hilfe",
    "Bilder und Audio sind inhärent mehrdeutig—die KI braucht Kontext, um zu wissen, welche Aspekte wichtig sind",
    "Multimodale Modelle können nur jeweils einen Eingabetyp verarbeiten",
    "Text-Prompts funktionieren nicht mit multimodalen Modellen"
  ]}
  correctIndex={1}
  explanation="Wenn du ein Bild betrachtest, weißt du sofort basierend auf deinen Zielen, was wichtig ist. Die KI hat diesen Kontext nicht—ein Foto von einem Wandriss könnte ein Ingenieurproblem, eine künstlerische Textur oder irrelevanter Hintergrund sein. Dein Prompt bestimmt, wie die KI die von dir bereitgestellten Medien interpretiert und darauf fokussiert."
/>
