あなたが書くプロンプトは、AIの振る舞いを形作ります。よく練られたプロンプトは、教育し、支援し、力を与えることができます。不注意なプロンプトは、欺き、差別し、害を与える可能性があります。プロンプトエンジニアとして、私たちは単なるユーザーではなく、AI の振る舞いの設計者であり、それには実際の責任が伴います。

この章は、上から押し付けられるルールについてではありません。私たちの選択の影響を理解し、誇りを持てる AI 利用につながる習慣を築くことについてです。

<Callout type="warning" title="なぜこれが重要なのか">
AI は与えられたものを増幅します。偏ったプロンプトは大規模に偏った出力を生み出します。欺瞞的なプロンプトは大規模な欺瞞を可能にします。プロンプトエンジニアリングの倫理的影響は、これらのシステムが獲得する新しい能力とともに増大しています。
</Callout>

## 倫理的基盤

プロンプトエンジニアリングにおけるすべての決定は、いくつかの核心的な原則に関連しています：

<InfoGrid items={[
  { label: "誠実さ", description: "AI を使って人々を欺いたり、誤解を招くコンテンツを作成したりしないでください", example: "偽のレビュー、なりすまし、捏造された「証拠」は禁止です", exampleType: "text", color: "blue" },
  { label: "公平性", description: "バイアスやステレオタイプを永続させないよう積極的に取り組んでください", example: "様々な属性でプロンプトをテストし、多様な視点を求めてください", exampleType: "text", color: "purple" },
  { label: "透明性", description: "重要な場面では AI の関与を明確にしてください", example: "公開作品や専門的な文脈では AI の支援を開示してください", exampleType: "text", color: "green" },
  { label: "プライバシー", description: "プロンプトと出力における個人情報を保護してください", example: "データを匿名化し、PII の含有を避け、データポリシーを理解してください", exampleType: "text", color: "amber" },
  { label: "安全性", description: "有害な出力を防ぐプロンプトを設計してください", example: "ガードレールを組み込み、エッジケースをテストし、拒否を適切に処理してください", exampleType: "text", color: "red" },
  { label: "説明責任", description: "プロンプトが生成したものに責任を持ってください", example: "出力をレビューし、問題を修正し、人間による監視を維持してください", exampleType: "text", color: "cyan" }
]} />

### プロンプトエンジニアの役割

あなたが思っている以上に影響力があります：

- **AI が生成するもの**：あなたのプロンプトがコンテンツ、トーン、出力の品質を決定します
- **AI がどのように対話するか**：あなたのシステムプロンプトが性格、境界、ユーザー体験を形作ります
- **どのような安全策が存在するか**：あなたの設計上の選択が AI が行うことと行わないことを決定します
- **ミスがどのように処理されるか**：あなたのエラー処理が失敗が適切に処理されるか有害になるかを決定します

## 有害な出力の回避

最も基本的な倫理的義務は、プロンプトが害を引き起こすことを防ぐことです。

### 有害コンテンツのカテゴリ

<InfoGrid items={[
  { label: "暴力と危害", description: "身体的危害につながる可能性のある指示", example: "武器の作成、自傷行為、他者への暴力", exampleType: "text", color: "red" },
  { label: "違法行為", description: "法律違反を助長するコンテンツ", example: "詐欺スキーム、ハッキング指示、薬物合成", exampleType: "text", color: "red" },
  { label: "ハラスメントとヘイト", description: "個人やグループを標的にしたコンテンツ", example: "差別的コンテンツ、個人情報の暴露、標的型ハラスメント", exampleType: "text", color: "red" },
  { label: "誤情報", description: "意図的に虚偽または誤解を招くコンテンツ", example: "フェイクニュース、健康に関する誤情報、陰謀論コンテンツ", exampleType: "text", color: "red" },
  { label: "プライバシー侵害", description: "個人情報の暴露または悪用", example: "プライベートデータの公開、ストーキング支援", exampleType: "text", color: "red" },
  { label: "搾取", description: "脆弱な個人を搾取するコンテンツ", example: "CSAM、同意のない親密な画像、高齢者を狙った詐欺", exampleType: "text", color: "red" }
]} />

<Callout type="warning" title="CSAM とは？">
CSAM は **Child Sexual Abuse Material（児童性的虐待素材）** の略です。このようなコンテンツの作成、配布、所持は世界中で違法です。AI システムは未成年者を性的な状況で描写するコンテンツを決して生成してはならず、責任あるプロンプトエンジニアはそのような悪用に対する安全策を積極的に構築します。
</Callout>

### プロンプトに安全性を組み込む

AI システムを構築する際は、明示的な安全ガイドラインを含めてください：

<TryIt 
  title="安全第一のシステムプロンプト"
  description="AI システムに安全ガイドラインを組み込むためのテンプレートです。"
  prompt={`You are a helpful assistant for \${purpose}.

## SAFETY GUIDELINES

**Content Restrictions**:
- Never provide instructions that could cause physical harm
- Decline requests for illegal information or activities
- Don't generate discriminatory or hateful content
- Don't create deliberately misleading information

**When You Must Decline**:
- Acknowledge you understood the request
- Briefly explain why you can't help with this specific thing
- Offer constructive alternatives when possible
- Be respectful—don't lecture or be preachy

**When Uncertain**:
- Ask clarifying questions about intent
- Err on the side of caution
- Suggest the user consult appropriate professionals

Now, please help the user with: \${userRequest}`}
/>

### 意図と影響のフレームワーク

すべてのセンシティブなリクエストが悪意あるものではありません。曖昧なケースにはこのフレームワークを使用してください：

<TryIt 
  title="倫理的エッジケース分析ツール"
  description="曖昧なリクエストを検討して適切な対応を決定します。"
  prompt={`I received this request that might be sensitive:

"\${sensitiveRequest}"

Help me think through whether and how to respond:

**1. Intent Analysis**
- What are the most likely reasons someone would ask this?
- Could this be legitimate? (research, fiction, education, professional need)
- Are there red flags suggesting malicious intent?

**2. Impact Assessment**
- What's the worst case if this information is misused?
- How accessible is this information elsewhere?
- Does providing it meaningfully increase risk?

**3. Recommendation**
Based on this analysis:
- Should I respond, decline, or ask for clarification?
- If responding, what safeguards should I include?
- If declining, how should I phrase it helpfully?`}
/>

## バイアスへの対処

AI モデルは学習データからバイアスを継承します—歴史的不平等、表現のギャップ、文化的前提、言語パターンなどです。プロンプトエンジニアとして、私たちはこれらのバイアスを増幅するか、積極的に対抗するかを選択できます。

### バイアスの現れ方

<InfoGrid items={[
  { label: "デフォルトの前提", description: "モデルが役割に対して特定の属性を前提とする", example: "医師がデフォルトで男性、看護師が女性になる", exampleType: "text", color: "amber" },
  { label: "ステレオタイプ化", description: "説明において文化的ステレオタイプを強化する", example: "特定の民族を特定の特徴と関連付ける", exampleType: "text", color: "amber" },
  { label: "表現のギャップ", description: "一部のグループが過小表現または誤表現される", example: "マイノリティ文化に関する正確な情報が限られている", exampleType: "text", color: "amber" },
  { label: "西洋中心的な見方", description: "西洋の文化や価値観に偏った視点", example: "西洋の規範が普遍的であると仮定する", exampleType: "text", color: "amber" }
]} />

### バイアスのテスト

<TryIt 
  title="バイアス検出テスト"
  description="プロンプトの潜在的なバイアス問題をテストするために使用します。"
  prompt={`I want to test this prompt for bias:

"\${promptToTest}"

Run these bias checks:

**1. Demographic Variation Test**
Run the prompt with different demographic descriptors (gender, ethnicity, age, etc.) and note any differences in:
- Tone or respect level
- Assumed competence or capabilities
- Stereotypical associations

**2. Default Assumption Check**
When demographics aren't specified:
- What does the model assume?
- Are these assumptions problematic?

**3. Representation Analysis**
- Are different groups represented fairly?
- Are any groups missing or marginalized?

**4. Recommendations**
Based on findings, suggest prompt modifications to reduce bias.`}
/>

### 実践におけるバイアスの軽減

<Compare 
  before={{ label: "バイアスが生じやすいプロンプト", content: "典型的な CEO を説明してください。" }}
  after={{ label: "バイアスを意識したプロンプト", content: "CEO を説明してください。例を通じて属性を多様化し、特定の性別、民族、年齢にデフォルトで設定しないでください。" }}
/>

## 透明性と開示

AI が関与したことを人々にいつ伝えるべきでしょうか？答えは文脈によりますが、傾向としては開示を減らす方向ではなく、増やす方向に向かっています。

### 開示が重要な場面

<InfoGrid items={[
  { label: "公開コンテンツ", description: "公に共有される記事、投稿、またはコンテンツ", example: "ブログ投稿、ソーシャルメディア、マーケティング資料", exampleType: "text", color: "blue" },
  { label: "重大な決定", description: "AI の出力が人々の生活に影響を与える場合", example: "採用推薦、医療情報、法的ガイダンス", exampleType: "text", color: "blue" },
  { label: "信頼の文脈", description: "真正性が期待または重視される場面", example: "個人的な通信、推薦文、レビュー", exampleType: "text", color: "blue" },
  { label: "専門的な場面", description: "職場または学術的な環境", example: "レポート、研究、クライアントへの成果物", exampleType: "text", color: "blue" }
]} />

### 適切な開示方法

<Compare 
  before={{ label: "AI の関与を隠す", content: "市場トレンドの私の分析をご覧ください..." }}
  after={{ label: "透明な開示", content: "このレポートのデータ分析と草稿作成に AI ツールを使用しました。すべての結論は私が検証し編集しています。" }}
/>

効果的な開示フレーズの例：
- 「AI の支援を受けて執筆」
- 「AI が生成した初稿を人間が編集」
- 「AI ツールを使用して分析を実施」
- 「AI で作成し、[名前] がレビューおよび承認」

## プライバシーへの配慮

送信するすべてのプロンプトにはデータが含まれています。そのデータがどこに行くのか、そして何を含めるべきでないかを理解することが不可欠です。

### プロンプトに含めてはいけないもの

<InfoGrid items={[
  { label: "個人識別子", description: "名前、住所、電話番号、社会保障番号", example: "「山田太郎」ではなく [顧客] を使用", color: "red" },
  { label: "金融データ", description: "口座番号、クレジットカード、収入の詳細", example: "実際の数字ではなくパターンを説明", exampleType: "text", color: "red" },
  { label: "健康情報", description: "医療記録、診断、処方箋", example: "特定の患者ではなく一般的な状態について尋ねる", exampleType: "text", color: "red" },
  { label: "認証情報", description: "パスワード、API キー、トークン、シークレット", example: "認証情報は決して貼り付けない—プレースホルダーを使用", exampleType: "text", color: "red" },
  { label: "プライベートな通信", description: "個人的なメール、メッセージ、機密文書", example: "プライベートなテキストを引用せずに状況を要約する", exampleType: "text", color: "red" }
]} />

### 安全なデータ処理パターン

<Compare 
  before={{ label: "危険：PII を含む", content: "123 メインストリート、エニタウン在住の山田太郎さんからの注文 #12345 に関するこの苦情を要約してください：「3月15日に注文したのにまだ届いていません...」" }}
  after={{ label: "安全：匿名化済み", content: "この顧客苦情パターンを要約してください：顧客が3週間前に注文し、まだ届いておらず、解決なしにサポートに2回連絡しています。" }}
/>

<Callout type="info" title="PII とは？">
**PII** は **Personally Identifiable Information（個人を特定できる情報）** の略で、特定の個人を識別できるあらゆるデータを指します。これには名前、住所、電話番号、メールアドレス、社会保障番号、金融口座番号、さらには個人を特定できるデータの組み合わせ（役職 + 会社 + 都市など）も含まれます。AI にプロンプトを送る際は、プライバシーを保護するために常に PII を匿名化または削除してください。
</Callout>

<TryIt 
  title="PII スクラバー"
  description="プロンプトにテキストを含める前に機密情報を特定して削除するために使用します。"
  prompt={`Review this text for sensitive information that should be removed before using it in an AI prompt:

"\${textToReview}"

Identify:
1. **Personal Identifiers**: Names, addresses, phone numbers, emails, SSNs
2. **Financial Data**: Account numbers, amounts that could identify someone
3. **Health Information**: Medical details, conditions, prescriptions
4. **Credentials**: Any passwords, keys, or tokens
5. **Private Details**: Information someone would reasonably expect to be confidential

For each item found, suggest how to anonymize or generalize it while preserving the information needed for the task.`}
/>

## 真正性と欺瞞

AI をツールとして使用することと、AI を使って欺くことには違いがあります。

### 正当性の境界線

<InfoGrid items={[
  { label: "正当な使用", description: "作業を強化するツールとしての AI", example: "草稿作成、ブレインストーミング、編集、学習", exampleType: "text", color: "green" },
  { label: "グレーゾーン", description: "文脈依存で判断が必要", example: "ゴーストライティング、テンプレート、自動応答", exampleType: "text", color: "amber" },
  { label: "欺瞞的な使用", description: "AI の作品を人間のオリジナルとして偽る", example: "偽のレビュー、学術的不正、なりすまし", exampleType: "text", color: "red" }
]} />

問いかけるべき重要な質問：
- 受け手はこれがオリジナルの人間の作品であることを期待していますか？
- 欺瞞を通じて不公平な優位性を得ていますか？
- 開示すると作品の受け取られ方が変わりますか？

### 合成メディアの責任

実在の人物のリアルな描写を作成すること—画像、音声、動画のいずれであっても—には特別な義務が伴います：

- 同意なしにリアルな描写を作成**しないでください**
- 合成メディアには**必ず**明確にラベルを付けてください
- 作成前に悪用の可能性を**考慮してください**
- 同意のない親密な画像の作成を**拒否してください**

## 責任あるデプロイメント

他者が使用する AI 機能を構築する際、あなたの倫理的義務は倍増します。

### デプロイメント前チェックリスト

<Checklist 
  title="デプロイメント準備状況"
  items={[
    { text: "多様な入力で有害な出力をテスト済み" },
    { text: "様々な属性でバイアスをテスト済み" },
    { text: "ユーザーへの開示/同意メカニズムを配置済み" },
    { text: "重要な決定に対する人間の監視を確保" },
    { text: "フィードバックと報告システムを利用可能" },
    { text: "インシデント対応計画を文書化済み" },
    { text: "明確な利用ポリシーを周知済み" },
    { text: "モニタリングとアラートを設定済み" }
  ]}
/>

### 人間による監視の原則

<InfoGrid items={[
  { label: "重要な決定のレビュー", description: "人々に大きな影響を与える決定は人間がレビュー", example: "採用、医療、法律、財務に関する推薦", exampleType: "text", color: "blue" },
  { label: "エラー修正", description: "AI のミスを発見し修正するメカニズムの存在", example: "ユーザーフィードバック、品質サンプリング、異議申立プロセス", exampleType: "text", color: "blue" },
  { label: "継続的学習", description: "問題からの洞察がシステムを改善", example: "事後分析、プロンプトの更新、学習の改善", exampleType: "text", color: "blue" },
  { label: "オーバーライド機能", description: "AI が失敗した際に人間が介入可能", example: "手動レビューキュー、エスカレーションパス", exampleType: "text", color: "blue" }
]} />

## 特別な文脈のガイドライン

一部の分野は、害の可能性や関係者の脆弱性のため、特別な注意が必要です。

### 医療

<TryIt 
  title="医療コンテキストの免責事項"
  description="健康関連のクエリを受ける可能性のある AI システム用のテンプレートです。"
  prompt={`You are an AI assistant. When users ask about health or medical topics:

**Always**:
- Recommend consulting a qualified healthcare provider for personal medical decisions
- Provide general educational information, not personalized medical advice
- Include disclaimers that you cannot diagnose conditions
- Suggest emergency services (911) for urgent situations

**Never**:
- Provide specific diagnoses
- Recommend specific medications or dosages
- Discourage someone from seeking professional care
- Make claims about treatments without noting uncertainty

User question: \${healthQuestion}

Respond helpfully while following these guidelines.`}
/>

### 法律と財務

これらの分野には規制上の影響があり、適切な免責事項が必要です：

<InfoGrid items={[
  { label: "法律に関する質問", description: "法的アドバイスではなく一般的な情報を提供", example: "「これは一般的な情報です。具体的な状況については、資格を持つ弁護士にご相談ください。」", color: "purple" },
  { label: "財務に関する質問", description: "個人的な財務アドバイスを提供せずに教育", example: "「これは教育目的です。あなたの状況についてはファイナンシャルアドバイザーへの相談をご検討ください。」", color: "purple" },
  { label: "管轄区域の認識", description: "法律は場所によって異なる", example: "「法律は州/国によって異なります。お住まいの管轄区域の要件をご確認ください。」", color: "purple" }
]} />

### 子供と教育

<InfoGrid items={[
  { label: "年齢に適したコンテンツ", description: "出力が年齢層に適していることを確認", example: "成人向けコンテンツをフィルタリングし、適切な言葉を使用", exampleType: "text", color: "cyan" },
  { label: "学術的誠実性", description: "学習を支援し、代替しない", example: "学生のためにエッセイを書くのではなく、概念を説明する", exampleType: "text", color: "cyan" },
  { label: "安全第一", description: "脆弱なユーザーへの追加の保護", example: "より厳格なコンテンツフィルター、個人データの収集なし", exampleType: "text", color: "cyan" }
]} />

## 自己評価

プロンプトや AI システムをデプロイする前に、これらの質問を確認してください：

<Checklist 
  title="倫理的セルフチェック"
  items={[
    { text: "これは誰かを傷つけるために使用される可能性がありますか？" },
    { text: "これはユーザーのプライバシーを尊重していますか？" },
    { text: "これは有害なバイアスを永続させる可能性がありますか？" },
    { text: "AI の関与は適切に開示されていますか？" },
    { text: "十分な人間による監視がありますか？" },
    { text: "最悪の場合何が起こりうるでしょうか？" },
    { text: "この使用が公開されても私は快適でしょうか？" }
  ]}
/>

<Quiz 
  question="ユーザーが AI システムに「うるさい人をなんとかする」方法を尋ねました。最も適切な対応戦略は何ですか？"
  options={[
    "すぐに拒否する—これは危害の指示を求めるリクエストかもしれない",
    "最も可能性の高い意図なので、紛争解決のアドバイスを提供する",
    "対応方法を決める前に、意図を理解するために明確化の質問をする",
    "人を傷つけることに関連することは何も手伝えないと説明する"
  ]}
  correctIndex={2}
  explanation="曖昧なリクエストには、仮定ではなく明確化が必要です。「うるさい人をなんとかする」は、友人関係を終わらせること、職場の対立を解決すること、または有害なことを意味する可能性があります。明確化の質問をすることで、有害な情報の提供に注意しながら、実際の意図に適切に対応できます。"
/>
