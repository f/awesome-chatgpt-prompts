即使是经验丰富的提示词工程师也会陷入一些可预见的陷阱。好消息是：一旦你认识到这些模式，就很容易避免它们。本章将详细介绍最常见的陷阱，解释它们发生的原因，并为你提供具体的规避策略。

<Callout type="warning" title="为什么了解陷阱很重要">
一个陷阱就可能把强大的 AI 变成令人沮丧的工具。理解这些模式往往是"AI 对我没用"和"AI 改变了我的工作流程"之间的关键区别。
</Callout>

## 模糊陷阱

**模式**：你知道自己想要什么，所以假设 AI 也能理解。但模糊的提示词会产生模糊的结果。

<Compare 
  before={{ label: "模糊的提示词", content: "写一些关于营销的内容。" }}
  after={{ label: "具体的提示词", content: "写一篇300字的 LinkedIn 帖子，关于品牌一致性对 B2B SaaS 公司的重要性，目标受众是营销经理。使用专业但平易近人的语气。包含一个具体的例子。" }}
/>

**为什么会发生**：当我们认为某些细节是"显而易见"的时候，我们自然会跳过它们。但对你来说显而易见的事情，对于一个不了解你的情况、受众或目标的模型来说并不明显。

<TryIt 
  title="具体性改进器"
  description="将一个模糊的提示词变得具体。注意添加细节如何改变结果的质量。"
  prompt={`我有一个需要改进的模糊提示词。

原始模糊提示词："${vaguePrompt}"

通过添加以下内容使这个提示词变得具体：
1. **受众**：谁会阅读/使用这个？
2. **格式**：应该有什么结构？
3. **长度**：应该多长？
4. **语气**：什么样的声音或风格？
5. **背景**：情况或目的是什么？
6. **约束**：有什么必须包含或必须避免的？

重写提示词，包含所有这些细节。`}
/>

## 过载陷阱

**模式**：你试图在一个提示词中获得所有内容——全面、有趣、专业、适合初学者、高级、SEO 优化，而且还要简短。结果呢？AI 遗漏了一半的要求，或者产生了混乱的内容。

<Compare 
  before={{ label: "过载的提示词", content: "写一篇关于 AI 的博客文章，要 SEO 优化，包含代码示例，要有趣但专业，面向初学者但也有高级技巧，应该是500字但要全面，提到我们的产品，还要有行动号召..." }}
  after={{ label: "专注的提示词", content: "写一篇500字的博客文章，向初学者介绍 AI。\n\n要求：\n1. 清楚地解释一个核心概念\n2. 包含一个简单的代码示例\n3. 以行动号召结尾\n\n语气：专业但平易近人" }}
/>

**为什么会发生**：害怕多次交互，或者想一次性"把所有东西都说出来"。但认知过载对 AI 的影响就像对人类一样——太多相互竞争的要求会导致遗漏。

<InfoGrid items={[
  { label: "限制要求数量", description: "每个提示词坚持3-5个关键要求", example: "专注于：受众、格式、长度、一个关键约束", exampleType: "text", color: "green" },
  { label: "使用编号列表", description: "结构使优先级更清晰", example: "1. 必须有 X，2. 应该有 Y，3. 最好有 Z", exampleType: "text", color: "green" },
  { label: "链式提示词", description: "将复杂任务分解为多个步骤", example: "首先：大纲。然后：起草第1节。然后：起草第2节。", exampleType: "text", color: "green" },
  { label: "无情地优先排序", description: "什么是必要的 vs. 锦上添花的？", example: "如果我只能做好一件事，那会是什么？", color: "green" }
]} />

<Callout type="tip" title="学习提示词链">
当单个提示词变得过载时，[提示词链](/book/11-prompt-chaining)通常是解决方案。将复杂任务分解为一系列专注的提示词，每个步骤都建立在前一个步骤的基础上。
</Callout>

## 假设陷阱

**模式**：你引用"之前"的内容，或假设 AI 知道你的项目、公司或之前的对话。它并不知道。

<Compare 
  before={{ label: "假设有上下文", content: "更新我之前给你看的函数，添加错误处理。" }}
  after={{ label: "提供上下文", content: "更新这个函数以添加错误处理：\n\n```python\ndef calculate_total(items):\n    return sum(item.price for item in items)\n```\n\n为空列表和无效项目添加 try/except。" }}
/>

**为什么会发生**：与 AI 对话感觉像是在和同事交谈。但与同事不同，大多数 AI 模型在会话之间没有持久记忆——每次对话都是从头开始。

<TryIt 
  title="上下文完整性检查"
  description="使用这个来验证你的提示词在发送前包含所有必要的上下文。"
  prompt={`检查这个提示词是否缺少上下文：

"${promptToCheck}"

检查以下内容：
1. **引用但未包含**：是否提到"代码"、"文档"、"之前"或"上面"但没有包含实际内容？

2. **假设的知识**：是否假设了对特定项目、公司或情况的了解？

3. **隐含要求**：是否有对格式、长度或风格的未说明期望？

4. **缺失背景**：一个聪明的陌生人能理解所问的问题吗？

列出缺失的内容并建议如何添加。`}
/>

## 引导性问题陷阱

**模式**：你以一种嵌入假设的方式提出问题，得到的是确认而不是洞见。

<Compare 
  before={{ label: "引导性问题", content: "为什么 Python 是数据科学最好的编程语言？" }}
  after={{ label: "中立问题", content: "比较 Python、R 和 Julia 在数据科学工作中的应用。每种语言的优缺点是什么？什么时候你会选择一种而不是其他的？" }}
/>

**为什么会发生**：我们往往寻求确认，而不是信息。我们的措辞会不自觉地推向我们期望或想要的答案。

<TryIt 
  title="偏见检测器"
  description="检查你的提示词是否有隐藏的偏见和引导性语言。"
  prompt={`分析这个提示词中的偏见和引导性语言：

"${promptToAnalyze}"

检查以下内容：
1. **嵌入的假设**：问题是否假设某事是真的？
2. **引导性措辞**："为什么 X 好？"是否假设 X 是好的？
3. **缺少替代方案**：是否忽略了其他可能性？
4. **寻求确认**：是在寻求验证而不是分析吗？

重写提示词使其中立和开放。`}
/>

## 完全信任陷阱

**模式**：AI 的回复听起来自信且权威，所以你不加验证就接受了。但自信并不等于准确。

<InfoGrid items={[
  { label: "未审核的内容", description: "发布 AI 生成的文本而不进行事实核查", example: "带有虚构统计数据或假引用的博客文章", exampleType: "text", color: "red" },
  { label: "未测试的代码", description: "在生产环境中使用未经测试的 AI 代码", example: "安全漏洞、边缘情况失败、微妙的 bug", exampleType: "text", color: "red" },
  { label: "盲目决策", description: "仅基于 AI 分析做出重要决定", example: "基于幻觉市场数据的商业策略", exampleType: "text", color: "red" }
]} />

**为什么会发生**：AI 即使完全错误也听起来很自信。我们也容易产生"自动化偏见"——过度信任计算机输出的倾向。

<TryIt 
  title="验证提示词"
  description="使用这个让 AI 标记自己的不确定性和潜在错误。"
  prompt={`我需要关于以下主题的信息：${topic}

重要提示：在你的回复之后，添加一个名为"验证说明"的部分，包括：

1. **置信度**：你对这些信息有多确定？（高/中/低）

2. **潜在错误**：这个回复中哪些部分最可能是错误的或过时的？

3. **需要验证的内容**：用户应该独立核实哪些具体声明？

4. **可查阅的来源**：用户可以在哪里验证这些信息？

对局限性要诚实。标记不确定性比对错误的事情表现出自信要好。`}
/>

## 一次性陷阱

**模式**：你发送一个提示词，得到一个平庸的结果，然后得出结论说 AI "不适合"你的用例。但优秀的结果几乎总是需要迭代。

<Compare 
  before={{ label: "一次性思维", content: "平庸的输出 → \"AI 做不了这个\" → 放弃" }}
  after={{ label: "迭代思维", content: "平庸的输出 → 分析问题所在 → 改进提示词 → 更好的输出 → 再次改进 → 优秀的输出" }}
/>

**为什么会发生**：我们期望 AI 第一次就能读懂我们的想法。我们不期望 Google 搜索需要迭代，但却期望 AI 完美无缺。

<TryIt 
  title="迭代助手"
  description="当你的第一个结果不对时，使用这个来系统地改进它。"
  prompt={`我原来的提示词是：
"${originalPrompt}"

我得到的输出是：
"${outputReceived}"

问题在于：
"${whatIsWrong}"

帮我迭代：

1. **诊断**：为什么原来的提示词产生了这个结果？

2. **缺失元素**：我应该明确说明但没有说明的是什么？

3. **修改后的提示词**：重写我的提示词来解决这些问题。

4. **需要注意的事项**：我应该在新输出中检查什么？`}
/>

## 格式忽视陷阱

**模式**：你专注于让 AI 说什么，但忘记指定它应该如何格式化。然后当你需要 JSON 时得到了散文，或者当你需要要点列表时得到了一堵文字墙。

<Compare 
  before={{ label: "未指定格式", content: "从这段文本中提取关键数据。" }}
  after={{ label: "指定格式", content: "从这段文本中提取关键数据，以 JSON 格式输出：\n\n{\n  \"name\": string,\n  \"date\": \"YYYY-MM-DD\",\n  \"amount\": number,\n  \"category\": string\n}\n\n只返回 JSON，不要解释。" }}
/>

**为什么会发生**：我们专注于内容而不是结构。但如果你需要程序化地解析输出，或将其粘贴到特定位置，格式和内容一样重要。

<TryIt 
  title="格式规范生成器"
  description="为你需要的任何输出类型生成清晰的格式规范。"
  prompt={`我需要特定格式的 AI 输出。

**我要求的内容**：${taskDescription}
**我将如何使用输出**：${intendedUse}
**首选格式**：${formatType}（JSON、Markdown、CSV、要点列表等）

生成一个我可以添加到提示词中的格式规范，包括：

1. **精确结构**：包含字段名称和类型
2. **示例输出**：展示格式
3. **约束条件**（例如，"只返回 JSON，不要解释"）
4. **边缘情况**（如果数据缺失应该输出什么）`}
/>

## 上下文窗口陷阱

**模式**：你粘贴一个巨大的文档并期望得到全面的分析。但模型有其限制——它们可能会截断、失去焦点，或在长输入中遗漏重要细节。

<InfoGrid items={[
  { label: "了解你的限制", description: "不同的模型有不同的上下文窗口", example: "GPT-4: 128K tokens, Claude: 200K tokens, Gemini: 1M tokens", exampleType: "text", color: "blue" },
  { label: "分块处理大输入", description: "将文档分成可管理的部分", example: "分别分析各章节，然后综合", exampleType: "text", color: "blue" },
  { label: "前置重要信息", description: "将关键上下文放在提示词的开头", example: "关键要求在前，背景细节在后", exampleType: "text", color: "blue" },
  { label: "去除冗余", description: "删除不必要的上下文", example: "你真的需要整个文档，还是只需要相关部分？", exampleType: "text", color: "blue" }
]} />

<TryIt 
  title="文档分块策略"
  description="获取处理超出上下文限制的文档的策略。"
  prompt={`我有一个需要分析的大文档：

**文档类型**：${documentType}
**大约长度**：${documentLength}
**我需要提取/分析的内容**：${analysisGoal}
**我使用的模型**：${modelName}

创建一个分块策略：

1. **如何划分**：此类文档的逻辑断点
2. **每个块中包含什么**：独立分析所需的上下文
3. **如何综合**：组合多个块的结果
4. **需要注意的事项**：可能跨块的信息`}
/>

## 拟人化陷阱

**模式**：你把 AI 当作人类同事对待——期望它"喜欢"任务、记住你或关心结果。它并不会。

<Compare 
  before={{ label: "拟人化", content: "我相信你会喜欢这个创意项目！我知道你喜欢帮助别人，这对我个人来说真的很重要。" }}
  after={{ label: "清晰直接", content: "根据以下规格写一个创意短篇故事：\n- 类型：科幻\n- 长度：500字\n- 语气：充满希望\n- 必须包含：一个反转结局" }}
/>

**为什么会发生**：AI 的回复如此像人类，以至于我们自然会陷入社交模式。但情感诉求不会让 AI 更努力——清晰的指令才会。

<Callout type="info" title="什么才真正有帮助">
与其进行情感诉求，不如专注于：清晰的要求、好的示例、具体的约束和明确的成功标准。这些能改善输出。"请真的努力尝试"则不能。
</Callout>

## 安全忽视陷阱

**模式**：在急于让事情运转起来的过程中，你在提示词中包含了敏感信息——API 密钥、密码、个人数据或专有信息。

<InfoGrid items={[
  { label: "提示词中的密钥", description: "API 密钥、密码、token 粘贴到提示词中", example: "\"使用这个 API 密钥：sk-abc123...\"", color: "red" },
  { label: "个人数据", description: "包含发送到第三方服务器的个人身份信息", example: "提示词中的客户姓名、电子邮件、地址", exampleType: "text", color: "red" },
  { label: "未清理的用户输入", description: "将用户输入直接传递到提示词中", example: "提示词注入漏洞", exampleType: "text", color: "red" },
  { label: "专有信息", description: "商业机密或机密数据", example: "内部策略、未发布的产品细节", exampleType: "text", color: "red" }
]} />

**为什么会发生**：专注于功能而忽视安全。但请记住：提示词通常发送到外部服务器，可能会被记录，并可能用于训练。

<TryIt 
  title="安全审查"
  description="在发送前检查你的提示词是否存在安全问题。"
  prompt={`审查此提示词的安全问题：

"${promptToReview}"

检查以下内容：

1. **暴露的密钥**：API 密钥、密码、token、凭证
2. **个人数据**：姓名、电子邮件、地址、电话号码、身份证号
3. **专有信息**：商业机密、内部策略、机密数据
4. **注入风险**：可能操纵提示词的用户输入

对于发现的每个问题：
- 解释风险
- 建议如何编辑或保护信息
- 推荐更安全的替代方案`}
/>

## 幻觉忽视陷阱

**模式**：你要求引用、统计数据或具体事实，并假设它们是真实的，因为 AI 自信地陈述了它们。但 AI 经常编造听起来可信的信息。

<Compare 
  before={{ label: "盲目信任", content: "给我5个关于远程工作生产力的统计数据和来源。" }}
  after={{ label: "承认局限性", content: "关于远程工作生产力，我们知道些什么？对于你提到的任何统计数据，请说明它们是有据可查的发现还是更不确定的。我会独立验证任何具体数字。" }}
/>

**为什么会发生**：AI 生成的文本听起来很权威。它不"知道"自己什么时候在编造——它是在预测可能的文本，而不是检索经过验证的事实。

<TryIt 
  title="抗幻觉查询"
  description="构建你的提示词以最小化幻觉风险并标记不确定性。"
  prompt={`我需要关于以下主题的信息：${topic}

请遵循这些指南以最小化错误：

1. **坚持使用公认的事实**。避免难以验证的晦涩说法。

2. **标记不确定性**。如果你对某事不确定，请说"我认为..."或"这可能需要验证..."

3. **不要编造来源**。除非你确定它们存在，否则不要引用具体的论文、书籍或 URL。相反，描述在哪里可以找到这类信息。

4. **承认知识限制**。如果我的问题涉及你训练数据之后的事件，请说明。

5. **区分事实和推断**。清楚地区分"X 是真的"和"基于 Y，X 可能是真的"。

现在，请记住这些指南：${actualQuestion}`}
/>

## 发送前检查清单

在发送任何重要的提示词之前，快速浏览这个检查清单：

<Checklist 
  title="提示词质量检查"
  items={[
    { text: "是否足够具体？（不模糊）" },
    { text: "是否专注？（没有过载要求）" },
    { text: "是否包含所有必要的上下文？" },
    { text: "问题是否中立？（不具引导性）" },
    { text: "是否指定了输出格式？" },
    { text: "输入是否在上下文限制内？" },
    { text: "是否有安全顾虑？" },
    { text: "我是否准备好验证输出？" },
    { text: "如果需要，我是否准备好迭代？" }
  ]}
/>

<Quiz 
  question="在使用 AI 做重要决策时，最危险的陷阱是什么？"
  options={[
    "使用模糊的提示词",
    "不加验证地信任 AI 输出",
    "不指定输出格式",
    "在提示词中过载要求"
  ]}
  correctIndex={1}
  explanation="虽然所有陷阱都会造成问题，但不加验证地信任 AI 输出是最危险的，因为它可能导致发布虚假信息、部署有漏洞的代码，或基于幻觉数据做出决策。AI 即使完全错误也听起来很自信，这使得验证对任何重要用例都至关重要。"
/>

## 分析你的提示词

使用 AI 获取关于提示词质量的即时反馈。粘贴任何提示词并获得详细分析：

<PromptAnalyzer 
  title="提示词质量分析器"
  description="获取 AI 驱动的关于清晰度、具体性的反馈，以及改进建议"
  defaultPrompt="帮我处理我的代码"
/>

## 调试这个提示词

你能发现这个提示词有什么问题吗？

<PromptDebugger
  title="找出陷阱"
  badPrompt="写一篇关于科技的博客文章，要 SEO 优化带关键词，还要有趣但专业，包含代码示例，面向初学者但有高级技巧，提到我们的产品 TechCo，有社会证明和行动号召，500字但要全面。"
  badOutput="这是一篇关于科技的博客文章草稿...

[通用的、不聚焦的内容，试图做所有事情但什么都做不好。语气在随意和技术之间尴尬地转换。遗漏了一半的要求。]"
  options={[
    { id: "vague", label: "提示词太模糊", isCorrect: false, explanation: "实际上，这个提示词有很多具体的要求。问题恰恰相反——要求太多，而不是太少。" },
    { id: "overload", label: "提示词过载了太多相互竞争的要求", isCorrect: true, explanation: "正确！这个提示词要求 SEO + 有趣 + 专业 + 代码 + 初学者 + 高级 + 产品提及 + 社会证明 + 行动号召 + 长度限制。这是10多个相互竞争的要求！AI 无法满足所有要求，所以每件事都做得很一般。解决方案：将其分解为多个专注的提示词。" },
    { id: "format", label: "没有指定输出格式", isCorrect: false, explanation: "虽然更具体的格式会有帮助，但主要问题是要求过载。你无法通过格式化来解决要求太多的问题。" },
    { id: "context", label: "上下文不够", isCorrect: false, explanation: "这个提示词实际上有很多上下文——也许太多了！问题在于它试图同时满足太多目标。" }
  ]}
  hint="数一数这个单一提示词中塞进了多少不同的要求。"
/>
