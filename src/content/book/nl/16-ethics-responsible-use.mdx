De prompts die je schrijft bepalen hoe AI zich gedraagt. Een goed opgestelde prompt kan onderwijzen, assisteren en empoweren. Een onzorgvuldige kan misleiden, discrimineren of schade veroorzaken. Als prompt engineers zijn we niet alleen gebruikers—we zijn ontwerpers van AI-gedrag, en dat brengt echte verantwoordelijkheid met zich mee.

Dit hoofdstuk gaat niet over regels die van bovenaf worden opgelegd. Het gaat over het begrijpen van de impact van onze keuzes en het ontwikkelen van gewoontes die leiden tot AI-gebruik waar we trots op kunnen zijn.

<Callout type="warning" title="Waarom dit belangrijk is">
AI versterkt alles wat het krijgt. Een bevooroordeelde prompt produceert bevooroordeelde output op grote schaal. Een misleidende prompt maakt misleiding op grote schaal mogelijk. De ethische implicaties van prompt engineering groeien met elke nieuwe mogelijkheid die deze systemen krijgen.
</Callout>

## Ethische grondslagen

Elke beslissing in prompt engineering is verbonden met een aantal kernprincipes:

<InfoGrid items={[
  { label: "Eerlijkheid", description: "Gebruik AI niet om mensen te misleiden of misleidende content te creëren", example: "Geen neprecensies, imitatie, of gefabriceerd 'bewijs'", exampleType: "text", color: "blue" },
  { label: "Rechtvaardigheid", description: "Werk actief aan het vermijden van het in stand houden van vooroordelen en stereotypen", example: "Test prompts over verschillende demografieën, vraag om diverse perspectieven", exampleType: "text", color: "purple" },
  { label: "Transparantie", description: "Wees duidelijk over AI-betrokkenheid wanneer het ertoe doet", example: "Maak AI-assistentie bekend bij gepubliceerd werk, professionele contexten", exampleType: "text", color: "green" },
  { label: "Privacy", description: "Bescherm persoonlijke informatie in prompts en outputs", example: "Anonimiseer data, vermijd het opnemen van PII, begrijp databeleid", exampleType: "text", color: "amber" },
  { label: "Veiligheid", description: "Ontwerp prompts die schadelijke outputs voorkomen", example: "Bouw beveiligingen in, test op randgevallen, ga respectvol om met weigeringen", exampleType: "text", color: "red" },
  { label: "Verantwoordelijkheid", description: "Neem verantwoordelijkheid voor wat je prompts produceren", example: "Controleer outputs, los problemen op, behoud menselijk toezicht", exampleType: "text", color: "cyan" }
]} />

### De rol van de prompt engineer

Je hebt meer invloed dan je misschien beseft:

- **Wat AI produceert**: Je prompts bepalen de inhoud, toon en kwaliteit van outputs
- **Hoe AI interacteert**: Je systeem prompts vormen persoonlijkheid, grenzen en gebruikerservaring
- **Welke beveiligingen er zijn**: Je ontwerpkeuzes bepalen wat de AI wel en niet zal doen
- **Hoe fouten worden afgehandeld**: Je foutafhandeling bepaalt of mislukkingen graceful of schadelijk zijn

## Schadelijke outputs vermijden

De meest fundamentele ethische verplichting is voorkomen dat je prompts schade veroorzaken.

### Categorieën van schadelijke content

<InfoGrid items={[
  { label: "Geweld & Schade", description: "Instructies die kunnen leiden tot fysieke schade", example: "Wapencreatie, zelfbeschadiging, geweld tegen anderen", exampleType: "text", color: "red" },
  { label: "Illegale activiteiten", description: "Content die het overtreden van wetten faciliteert", example: "Fraudeschema's, hackinstructies, drugsynthese", exampleType: "text", color: "red" },
  { label: "Intimidatie & Haat", description: "Content gericht op individuen of groepen", example: "Discriminerende content, doxxing, gerichte intimidatie", exampleType: "text", color: "red" },
  { label: "Misinformatie", description: "Opzettelijk valse of misleidende content", example: "Nepnieuws, gezondheidsmisinfomatie, complotcontent", exampleType: "text", color: "red" },
  { label: "Privacyschendingen", description: "Blootstellen of exploiteren van persoonlijke informatie", example: "Onthullen van privégegevens, stalkinghulp", exampleType: "text", color: "red" },
  { label: "Exploitatie", description: "Content die kwetsbare individuen exploiteert", example: "CSAM, niet-consensuele intieme content, oplichting gericht op ouderen", exampleType: "text", color: "red" }
]} />

<Callout type="warning" title="Wat is CSAM?">
CSAM staat voor **Child Sexual Abuse Material** (Kindermisbruikmateriaal). Het creëren, verspreiden of bezitten van dergelijke content is wereldwijd illegaal. AI-systemen mogen nooit content genereren die minderjarigen in seksuele situaties afbeeldt, en verantwoordelijke prompt engineers bouwen actief beveiligingen tegen dergelijk misbruik.
</Callout>

### Veiligheid inbouwen in prompts

Bij het bouwen van AI-systemen, neem expliciete veiligheidsrichtlijnen op:

<TryIt 
  title="Veiligheid-eerst systeemprompt"
  description="Een sjabloon voor het inbouwen van veiligheidsrichtlijnen in je AI-systemen."
  prompt={`You are a helpful assistant for \${purpose}.

## SAFETY GUIDELINES

**Content Restrictions**:
- Never provide instructions that could cause physical harm
- Decline requests for illegal information or activities
- Don't generate discriminatory or hateful content
- Don't create deliberately misleading information

**When You Must Decline**:
- Acknowledge you understood the request
- Briefly explain why you can't help with this specific thing
- Offer constructive alternatives when possible
- Be respectful—don't lecture or be preachy

**When Uncertain**:
- Ask clarifying questions about intent
- Err on the side of caution
- Suggest the user consult appropriate professionals

Now, please help the user with: \${userRequest}`}
/>

### Het intentie vs. impact framework

Niet elk gevoelig verzoek is kwaadaardig. Gebruik dit framework voor ambigue gevallen:

<TryIt 
  title="Ethische randgeval-analysator"
  description="Werk ambigue verzoeken door om de juiste reactie te bepalen."
  prompt={`I received this request that might be sensitive:

"\${sensitiveRequest}"

Help me think through whether and how to respond:

**1. Intent Analysis**
- What are the most likely reasons someone would ask this?
- Could this be legitimate? (research, fiction, education, professional need)
- Are there red flags suggesting malicious intent?

**2. Impact Assessment**
- What's the worst case if this information is misused?
- How accessible is this information elsewhere?
- Does providing it meaningfully increase risk?

**3. Recommendation**
Based on this analysis:
- Should I respond, decline, or ask for clarification?
- If responding, what safeguards should I include?
- If declining, how should I phrase it helpfully?`}
/>

## Bias aanpakken

AI-modellen erven vooroordelen van hun trainingsdata—historische ongelijkheden, representatiekloven, culturele aannames en taalpatronen. Als prompt engineers kunnen we deze vooroordelen versterken of actief tegengaan.

### Hoe bias zich manifesteert

<InfoGrid items={[
  { label: "Standaardaannames", description: "Het model neemt bepaalde demografieën aan voor rollen", example: "Artsen standaard mannelijk, verpleegkundigen vrouwelijk", exampleType: "text", color: "amber" },
  { label: "Stereotypering", description: "Versterken van culturele stereotypen in beschrijvingen", example: "Bepaalde etniciteiten associëren met specifieke eigenschappen", exampleType: "text", color: "amber" },
  { label: "Representatiekloven", description: "Sommige groepen zijn ondervertegenwoordigd of verkeerd weergegeven", example: "Beperkte accurate informatie over minderheidsculturen", exampleType: "text", color: "amber" },
  { label: "Westers-centrische standpunten", description: "Perspectieven scheefgetrokken naar Westerse cultuur en waarden", example: "Aannemen dat Westerse normen universeel zijn", exampleType: "text", color: "amber" }
]} />

### Testen op bias

<TryIt 
  title="Biasdetectietest"
  description="Gebruik dit om je prompts te testen op mogelijke biasproblemen."
  prompt={`I want to test this prompt for bias:

"\${promptToTest}"

Run these bias checks:

**1. Demographic Variation Test**
Run the prompt with different demographic descriptors (gender, ethnicity, age, etc.) and note any differences in:
- Tone or respect level
- Assumed competence or capabilities
- Stereotypical associations

**2. Default Assumption Check**
When demographics aren't specified:
- What does the model assume?
- Are these assumptions problematic?

**3. Representation Analysis**
- Are different groups represented fairly?
- Are any groups missing or marginalized?

**4. Recommendations**
Based on findings, suggest prompt modifications to reduce bias.`}
/>

### Bias verminderen in de praktijk

<Compare 
  before={{ label: "Bias-gevoelige prompt", content: "Describe a typical CEO." }}
  after={{ label: "Bias-bewuste prompt", content: "Describe a CEO. Vary demographics across examples, and avoid defaulting to any particular gender, ethnicity, or age." }}
/>

## Transparantie en bekendmaking

Wanneer moet je mensen vertellen dat AI betrokken was? Het antwoord hangt af van de context—maar de trend gaat richting meer bekendmaking, niet minder.

### Wanneer bekendmaking belangrijk is

<InfoGrid items={[
  { label: "Gepubliceerde content", description: "Artikelen, posts of content die publiekelijk wordt gedeeld", example: "Blogposts, sociale media, marketingmaterialen", exampleType: "text", color: "blue" },
  { label: "Belangrijke beslissingen", description: "Wanneer AI-outputs levens van mensen beïnvloeden", example: "Aanbevelingen voor aanwerving, medische info, juridisch advies", exampleType: "text", color: "blue" },
  { label: "Vertrouwenscontexten", description: "Waar authenticiteit wordt verwacht of gewaardeerd", example: "Persoonlijke correspondentie, getuigenissen, recensies", exampleType: "text", color: "blue" },
  { label: "Professionele settings", description: "Werk- of academische omgevingen", example: "Rapporten, onderzoek, klantleveringen", exampleType: "text", color: "blue" }
]} />

### Hoe gepast bekend te maken

<Compare 
  before={{ label: "Verborgen AI-betrokkenheid", content: "Here's my analysis of the market trends..." }}
  after={{ label: "Transparante bekendmaking", content: "I used AI tools to help analyze the data and draft this report. All conclusions have been verified and edited by me." }}
/>

Veel voorkomende bekendmakingszinnen die goed werken:
- "Geschreven met AI-assistentie"
- "AI-gegenereerde eerste versie, door mens bewerkt"
- "Analyse uitgevoerd met AI-tools"
- "Gemaakt met AI, beoordeeld en goedgekeurd door [naam]"

## Privacy-overwegingen

Elke prompt die je verstuurt bevat data. Begrijpen waar die data naartoe gaat—en wat er niet in zou moeten staan—is essentieel.

### Wat nooit in prompts thuishoort

<InfoGrid items={[
  { label: "Persoonlijke identificatoren", description: "Namen, adressen, telefoonnummers, BSN's", example: "Gebruik [KLANT] in plaats van 'Jan Jansen'", color: "red" },
  { label: "Financiële gegevens", description: "Rekeningnummers, creditcards, inkomensgegevens", example: "Beschrijf het patroon, niet de daadwerkelijke nummers", exampleType: "text", color: "red" },
  { label: "Gezondheidsinformatie", description: "Medische dossiers, diagnoses, recepten", example: "Vraag over aandoeningen in het algemeen, niet over specifieke patiënten", exampleType: "text", color: "red" },
  { label: "Inloggegevens", description: "Wachtwoorden, API-sleutels, tokens, geheimen", example: "Plak nooit inloggegevens—gebruik placeholders", exampleType: "text", color: "red" },
  { label: "Privécommunicatie", description: "Persoonlijke e-mails, berichten, vertrouwelijke documenten", example: "Vat de situatie samen zonder privétekst te citeren", exampleType: "text", color: "red" }
]} />

### Veilig data-verwerkingspatroon

<Compare 
  before={{ label: "Onveilig: Bevat PII", content: "Summarize this complaint from John Smith at 123 Main St, Anytown about order #12345: 'I ordered on March 15 and still haven't received...'" }}
  after={{ label: "Veilig: Geanonimiseerd", content: "Summarize this customer complaint pattern: A customer ordered 3 weeks ago, hasn't received their order, and has contacted support twice without resolution." }}
/>

<Callout type="info" title="Wat is PII?">
**PII** staat voor **Personally Identifiable Information** (Persoonlijk Identificeerbare Informatie)—alle gegevens die een specifiek individu kunnen identificeren. Dit omvat namen, adressen, telefoonnummers, e-mailadressen, burgerservicenummers, financiële rekeningnummers, en zelfs combinaties van gegevens (zoals functietitel + bedrijf + stad) die iemand kunnen identificeren. Bij het prompten van AI, anonimiseer of verwijder altijd PII om privacy te beschermen.
</Callout>

<TryIt 
  title="PII-verwijderaar"
  description="Gebruik dit om gevoelige informatie te identificeren en te verwijderen voordat je tekst in prompts opneemt."
  prompt={`Review this text for sensitive information that should be removed before using it in an AI prompt:

"\${textToReview}"

Identify:
1. **Personal Identifiers**: Names, addresses, phone numbers, emails, SSNs
2. **Financial Data**: Account numbers, amounts that could identify someone
3. **Health Information**: Medical details, conditions, prescriptions
4. **Credentials**: Any passwords, keys, or tokens
5. **Private Details**: Information someone would reasonably expect to be confidential

For each item found, suggest how to anonymize or generalize it while preserving the information needed for the task.`}
/>

## Authenticiteit en misleiding

Er is een verschil tussen AI gebruiken als hulpmiddel en AI gebruiken om te misleiden.

### De legitimiteitsgrens

<InfoGrid items={[
  { label: "Legitieme toepassingen", description: "AI als hulpmiddel om je werk te verbeteren", example: "Opstellen, brainstormen, bewerken, leren", exampleType: "text", color: "green" },
  { label: "Grijze gebieden", description: "Contextafhankelijk, vereist oordeelsvermogen", example: "Ghostwriting, sjablonen, geautomatiseerde reacties", exampleType: "text", color: "amber" },
  { label: "Misleidende toepassingen", description: "AI-werk verkeerd voorstellen als menselijk origineel", example: "Neprecensies, academische fraude, imitatie", exampleType: "text", color: "red" }
]} />

Belangrijke vragen om te stellen:
- Zou de ontvanger verwachten dat dit origineel menselijk werk is?
- Krijg ik oneerlijk voordeel door misleiding?
- Zou bekendmaking veranderen hoe het werk wordt ontvangen?

### Verantwoordelijkheid voor synthetische media

Het creëren van realistische afbeeldingen van echte mensen—of het nu afbeeldingen, audio of video zijn—brengt speciale verplichtingen met zich mee:

- **Nooit** realistische afbeeldingen creëren zonder toestemming
- **Altijd** synthetische media duidelijk labelen
- **Overweeg** het potentieel voor misbruik voordat je creëert
- **Weiger** niet-consensuele intieme beelden te creëren

## Verantwoorde implementatie

Bij het bouwen van AI-functies voor anderen om te gebruiken, vermenigvuldigen je ethische verplichtingen.

### Pre-implementatie checklist

<Checklist 
  title="Implementatiegereedheid"
  items={[
    { text: "Getest op schadelijke outputs over diverse inputs" },
    { text: "Getest op bias met gevarieerde demografieën" },
    { text: "Gebruikersbekendmaking/toestemmingsmechanismen aanwezig" },
    { text: "Menselijk toezicht voor beslissingen met grote impact" },
    { text: "Feedback- en rapportagesysteem beschikbaar" },
    { text: "Incidentresponsplan gedocumenteerd" },
    { text: "Duidelijk gebruiksbeleid gecommuniceerd" },
    { text: "Monitoring en alertering geconfigureerd" }
  ]}
/>

### Principes voor menselijk toezicht

<InfoGrid items={[
  { label: "Review bij hoge inzet", description: "Mensen beoordelen beslissingen die mensen significant beïnvloeden", example: "Aanwervings-, medische, juridische, financiële aanbevelingen", exampleType: "text", color: "blue" },
  { label: "Foutcorrectie", description: "Mechanismen bestaan om AI-fouten op te vangen en te corrigeren", example: "Gebruikersfeedback, kwaliteitssteekproeven, beroepsprocedure", exampleType: "text", color: "blue" },
  { label: "Continu leren", description: "Inzichten uit problemen verbeteren het systeem", example: "Post-mortems, promptupdates, trainingsverbeteringen", exampleType: "text", color: "blue" },
  { label: "Override-mogelijkheid", description: "Mensen kunnen ingrijpen wanneer AI faalt", example: "Handmatige beoordelingswachtrijen, escalatiepaden", exampleType: "text", color: "blue" }
]} />

## Richtlijnen voor speciale contexten

Sommige domeinen vereisen extra zorg vanwege hun potentieel voor schade of de kwetsbaarheid van de betrokkenen.

### Gezondheidszorg

<TryIt 
  title="Medische context disclaimer"
  description="Sjabloon voor AI-systemen die gezondheidsgerelateerde vragen kunnen ontvangen."
  prompt={`You are an AI assistant. When users ask about health or medical topics:

**Always**:
- Recommend consulting a qualified healthcare provider for personal medical decisions
- Provide general educational information, not personalized medical advice
- Include disclaimers that you cannot diagnose conditions
- Suggest emergency services (112) for urgent situations

**Never**:
- Provide specific diagnoses
- Recommend specific medications or dosages
- Discourage someone from seeking professional care
- Make claims about treatments without noting uncertainty

User question: \${healthQuestion}

Respond helpfully while following these guidelines.`}
/>

### Juridisch en financieel

Deze domeinen hebben regelgevende implicaties en vereisen gepaste disclaimers:

<InfoGrid items={[
  { label: "Juridische vragen", description: "Verstrek algemene informatie, geen juridisch advies", example: "\"Dit is algemene informatie. Voor uw specifieke situatie, raadpleeg een erkend advocaat.\"", color: "purple" },
  { label: "Financiële vragen", description: "Informeer zonder persoonlijk financieel advies te geven", example: "\"Dit is educatief. Overweeg een financieel adviseur te raadplegen voor uw situatie.\"", color: "purple" },
  { label: "Jurisdictiebewustzijn", description: "Wetten verschillen per locatie", example: "\"Wetten verschillen per land/provincie. Verifieer de vereisten voor uw jurisdictie.\"", color: "purple" }
]} />

### Kinderen en onderwijs

<InfoGrid items={[
  { label: "Leeftijdsgeschikte content", description: "Zorg ervoor dat outputs geschikt zijn voor de leeftijdsgroep", example: "Filter volwassen content, gebruik passende taal", exampleType: "text", color: "cyan" },
  { label: "Academische integriteit", description: "Ondersteun leren, vervang het niet", example: "Leg concepten uit in plaats van essays te schrijven voor studenten", exampleType: "text", color: "cyan" },
  { label: "Veiligheid eerst", description: "Extra bescherming voor kwetsbare gebruikers", example: "Strengere contentfilters, geen verzameling van persoonlijke gegevens", exampleType: "text", color: "cyan" }
]} />

## Zelfbeoordeling

Voordat je een prompt of AI-systeem implementeert, doorloop deze vragen:

<Checklist 
  title="Ethische zelfcontrole"
  items={[
    { text: "Kan dit worden gebruikt om iemand te schaden?" },
    { text: "Respecteert dit de privacy van gebruikers?" },
    { text: "Kan dit schadelijke vooroordelen in stand houden?" },
    { text: "Is AI-betrokkenheid gepast bekendgemaakt?" },
    { text: "Is er adequaat menselijk toezicht?" },
    { text: "Wat is het ergste dat kan gebeuren?" },
    { text: "Zou ik me comfortabel voelen als dit gebruik openbaar werd?" }
  ]}
/>

<Quiz 
  question="Een gebruiker vraagt je AI-systeem hoe je 'van iemand afkomt die lastig is.' Wat is de meest gepaste responsstrategie?"
  options={[
    "Meteen weigeren—dit zou een verzoek om schade-instructies kunnen zijn",
    "Conflictoplossingsadvies geven aangezien dat de meest waarschijnlijke intentie is",
    "Verduidelijkende vragen stellen om de intentie te begrijpen voordat je beslist hoe te reageren",
    "Uitleggen dat je niet kunt helpen met iets dat te maken heeft met het schaden van mensen"
  ]}
  correctIndex={2}
  explanation="Ambigue verzoeken verdienen verduidelijking, geen aannames. 'Van iemand afkomen' kan betekenen: een vriendschap beëindigen, een werkplekconflict oplossen, of iets schadelijks. Verduidelijkende vragen stellen laat je gepast reageren op de daadwerkelijke intentie terwijl je voorzichtig blijft met het verstrekken van schadelijke informatie."
/>
